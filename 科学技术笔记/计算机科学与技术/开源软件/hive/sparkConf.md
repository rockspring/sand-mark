sparkConf = {HashMap@8593}  size = 157
 0 = {HashMap$Node@8597} "spark.hadoop.hbase.ipc.server.callqueue.handler.factor" -> "0.1"
 1 = {HashMap$Node@8598} "spark.hadoop.hbase.thrift.maxWorkerThreads" -> "1000"
 2 = {HashMap$Node@8599} "spark.hadoop.hbase.regionserver.regionSplitLimit" -> "1000"
 3 = {HashMap$Node@8600} "spark.hadoop.hbase.security.exec.permission.checks" -> "false"
 4 = {HashMap$Node@8601} "spark.hadoop.hbase.regionserver.catalog.timeout" -> "600000"
 5 = {HashMap$Node@8602} "spark.hadoop.hbase.rest.port" -> "8080"
 6 = {HashMap$Node@8603} "spark.hadoop.hbase.security.authentication" -> "simple"
 7 = {HashMap$Node@8604} "hive.spark.client.connect.timeout" -> "1000"
 8 = {HashMap$Node@8605} "spark.hadoop.zookeeper.znode.acl.parent" -> "acl"
 9 = {HashMap$Node@8606} "spark.hadoop.hbase.zookeeper.useMulti" -> "true"
 10 = {HashMap$Node@8607} "spark.hadoop.hbase.regionserver.region.split.policy" -> "org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy"
 11 = {HashMap$Node@8608} "spark.hadoop.hbase.zookeeper.property.clientPort" -> "2181"
 12 = {HashMap$Node@8609} "spark.hadoop.hbase.auth.token.max.lifetime" -> "604800000"
 13 = {HashMap$Node@8610} "spark.hadoop.hbase.status.listener.class" -> "org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener"
 14 = {HashMap$Node@8611} "spark.hadoop.hbase.bulkload.staging.dir" -> "/user/shg4873/hbase-staging"
 15 = {HashMap$Node@8612} "spark.hadoop.hbase.cluster.distributed" -> "false"
 16 = {HashMap$Node@8613} "spark.hadoop.hbase.status.multicast.address.ip" -> "226.1.1.3"
 17 = {HashMap$Node@8614} "spark.hadoop.hbase.server.thread.wakefrequency" -> "10000"
 18 = {HashMap$Node@8615} "spark.hadoop.hbase.rest.readonly" -> "false"
 19 = {HashMap$Node@8616} "spark.hadoop.hbase.config.read.zookeeper.config" -> "false"
 20 = {HashMap$Node@8617} "spark.hadoop.hbase.client.retries.number" -> "35"
 21 = {HashMap$Node@8618} "spark.hadoop.hbase.lease.recovery.dfs.timeout" -> "64000"
 22 = {HashMap$Node@8619} "spark.hadoop.hbase.fs.tmp.dir" -> "/user/shg4873/hbase-staging"
 23 = {HashMap$Node@8620} "spark.hadoop.hbase.region.replica.replication.enabled" -> "false"
 24 = {HashMap$Node@8621} "spark.hadoop.hbase.regionserver.info.port" -> "16030"
 25 = {HashMap$Node@8622} "spark.hadoop.hbase.client.scanner.caching" -> "2147483647"
 26 = {HashMap$Node@8623} "spark.hadoop.hbase.regionserver.optionalcacheflushinterval" -> "3600000"
 27 = {HashMap$Node@8624} "spark.hadoop.hbase.regionserver.logroll.errors.tolerated" -> "2"
 28 = {HashMap$Node@8625} "spark.app.name" -> "Hive on Spark"
 29 = {HashMap$Node@8626} "spark.hadoop.hbase.tmp.dir" -> "/tmp/hbase-shg4873"
 30 = {HashMap$Node@8627} "spark.hadoop.hbase.coprocessor.enabled" -> "true"
 31 = {HashMap$Node@8628} "spark.hadoop.hbase.master.info.port" -> "16010"
 32 = {HashMap$Node@8629} "hive.spark.client.server.connect.timeout" -> "90000"
 33 = {HashMap$Node@8630} "spark.hadoop.hbase.hregion.max.filesize" -> "10737418240"
 34 = {HashMap$Node@8631} "spark.hadoop.hbase.client.max.perregion.tasks" -> "1"
 35 = {HashMap$Node@8632} "spark.hadoop.hbase.metrics.showTableName" -> "true"
 36 = {HashMap$Node@8633} "spark.hadoop.hbase.http.filter.initializers" -> "org.apache.hadoop.hbase.http.lib.StaticUserWebFilter"
 37 = {HashMap$Node@8634} "spark.hadoop.hbase.hregion.majorcompaction" -> "604800000"
 38 = {HashMap$Node@8635} "spark.hadoop.hbase.master.info.bindAddress" -> "0.0.0.0"
 39 = {HashMap$Node@8636} "hive.spark.client.secret.bits" -> "256"
 40 = {HashMap$Node@8637} "spark.hadoop.fs.defaultFS" -> "hdfs://127.0.0.1:9000"
 41 = {HashMap$Node@8638} "spark.hadoop.hbase.defaults.for.version.skip" -> "false"
 42 = {HashMap$Node@8639} "spark.hadoop.hbase.dynamic.jars.dir" -> "/tmp/hbase-shg4873/hbase/lib"
 43 = {HashMap$Node@8640} "spark.hadoop.hbase.snapshot.enabled" -> "true"
 44 = {HashMap$Node@8641} "spark.hadoop.hbase.zookeeper.dns.nameserver" -> "default"
 45 = {HashMap$Node@8642} "spark.hadoop.hbase.hstore.flusher.count" -> "2"
 46 = {HashMap$Node@8643} "spark.hadoop.hbase.http.max.threads" -> "10"
 47 = {HashMap$Node@8644} "spark.hadoop.hbase.storescanner.parallel.seek.threads" -> "10"
 48 = {HashMap$Node@8645} "spark.hadoop.hbase.zookeeper.property.syncLimit" -> "5"
 49 = {HashMap$Node@8646} "spark.hadoop.hbase.master.logcleaner.plugins" -> "org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner"
 50 = {HashMap$Node@8647} "spark.hadoop.hbase.zookeeper.property.initLimit" -> "10"
 51 = {HashMap$Node@8648} "spark.hadoop.hbase.http.staticuser.user" -> "dr.stack"
 52 = {HashMap$Node@8649} "spark.hadoop.hbase.snapshot.restore.take.failsafe.snapshot" -> "true"
 53 = {HashMap$Node@8650} "spark.kryo.referenceTracking" -> "false"
 54 = {HashMap$Node@8651} "spark.hadoop.hbase.master.port" -> "16000"
 55 = {HashMap$Node@8652} "spark.hadoop.hbase.dfs.client.read.shortcircuit.buffer.size" -> "131072"
 56 = {HashMap$Node@8653} "spark.hadoop.hbase.rpc.shortoperation.timeout" -> "10000"
 57 = {HashMap$Node@8654} "spark.hadoop.hbase.status.multicast.address.port" -> "16100"
 58 = {HashMap$Node@8655} "spark.hadoop.hbase.thrift.htablepool.size.max" -> "1000"
 59 = {HashMap$Node@8656} "spark.hadoop.hbase.regionserver.hlog.writer.impl" -> "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter"
 60 = {HashMap$Node@8657} "spark.hadoop.hbase.ipc.client.fallback-to-simple-auth-allowed" -> "false"
 61 = {HashMap$Node@8658} "spark.hadoop.hbase.coprocessor.user.enabled" -> "true"
 62 = {HashMap$Node@8659} "spark.hadoop.hbase.zookeeper.leaderport" -> "3888"
 63 = {HashMap$Node@8660} "spark.hadoop.hbase.client.pause" -> "100"
 64 = {HashMap$Node@8661} "spark.hadoop.hbase.online.schema.update.enable" -> "true"
 65 = {HashMap$Node@8662} "spark.hadoop.hbase.master.hfilecleaner.plugins" -> "org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner"
 66 = {HashMap$Node@8663} "spark.hadoop.hbase.table.max.rowsize" -> "1073741824"
 67 = {HashMap$Node@8664} "spark.hadoop.hbase.rest.threads.min" -> "2"
 68 = {HashMap$Node@8665} "spark.hadoop.hbase.replication.rpc.codec" -> "org.apache.hadoop.hbase.codec.KeyValueCodecWithTags"
 69 = {HashMap$Node@8666} "spark.hadoop.zookeeper.znode.rootserver" -> "root-region-server"
 70 = {HashMap$Node@8667} "spark.hadoop.hbase.zookeeper.peerport" -> "2888"
 71 = {HashMap$Node@8668} "spark.hadoop.hbase.rootdir" -> "/tmp/hbase-shg4873/hbase"
 72 = {HashMap$Node@8669} "spark.hadoop.hbase.rest.filter.classes" -> "org.apache.hadoop.hbase.rest.filter.GzipFilter"
 73 = {HashMap$Node@8670} "spark.hadoop.hbase.hstore.blockingStoreFiles" -> "10"
 74 = {HashMap$Node@8671} "spark.hadoop.hbase.client.scanner.timeout.period" -> "60000"
 75 = {HashMap$Node@8672} "spark.hadoop.hbase.hstore.compaction.kv.max" -> "10"
 76 = {HashMap$Node@8673} "spark.hadoop.hbase.server.scanner.max.result.size" -> "104857600"
 77 = {HashMap$Node@8674} "spark.hadoop.hbase.regionserver.dns.nameserver" -> "default"
 78 = {HashMap$Node@8675} "spark.hadoop.hbase.rest.threads.max" -> "100"
 79 = {HashMap$Node@8676} "spark.hadoop.hbase.defaults.for.version" -> "1.1.1"
 80 = {HashMap$Node@8677} "spark.hadoop.hbase.balancer.period" -> "300000"
 81 = {HashMap$Node@8678} "spark.hadoop.hbase.master.infoserver.redirect" -> "true"
 82 = {HashMap$Node@8679} "spark.hadoop.hbase.rootdir.perms" -> "700"
 83 = {HashMap$Node@8680} "spark.hadoop.hbase.lease.recovery.timeout" -> "900000"
 84 = {HashMap$Node@8681} "spark.hadoop.hbase.client.max.total.tasks" -> "100"
 85 = {HashMap$Node@8682} "spark.hadoop.hbase.server.compactchecker.interval.multiplier" -> "1000"
 86 = {HashMap$Node@8683} "spark.hadoop.hbase.regionserver.port" -> "16020"
 87 = {HashMap$Node@8684} "spark.hadoop.hbase.regionserver.handler.count" -> "30"
 88 = {HashMap$Node@8685} "spark.hadoop.hbase.hregion.memstore.block.multiplier" -> "4"
 89 = {HashMap$Node@8686} "spark.hadoop.hbase.server.versionfile.writeattempts" -> "3"
 90 = {HashMap$Node@8687} "spark.hadoop.hbase.hstore.compactionThreshold" -> "3"
 91 = {HashMap$Node@8688} "spark.hadoop.hbase.hstore.bytes.per.checksum" -> "16384"
 92 = {HashMap$Node@8689} "spark.hadoop.hbase.hregion.percolumnfamilyflush.size.lower.bound" -> "16777216"
 93 = {HashMap$Node@8690} "spark.hadoop.hbase.regionserver.storefile.refresh.period" -> "0"
 94 = {HashMap$Node@8691} "spark.hadoop.hbase.hstore.blockingWaitTime" -> "90000"
 95 = {HashMap$Node@8692} "spark.hadoop.hbase.master.distributed.log.replay" -> "false"
 96 = {HashMap$Node@8693} "spark.hadoop.hbase.local.dir" -> "/tmp/hbase-shg4873/local/"
 97 = {HashMap$Node@8694} "spark.hadoop.hbase.regionserver.thrift.framed.max_frame_size_in_mb" -> "2"
 98 = {HashMap$Node@8695} "spark.hadoop.hbase.client.write.buffer" -> "2097152"
 99 = {HashMap$Node@8696} "spark.hadoop.hbase.data.umask.enable" -> "false"
 100 = {HashMap$Node@8867} "spark.hadoop.hbase.data.umask" -> "000"
 101 = {HashMap$Node@8868} "spark.hadoop.hbase.client.localityCheck.threadPoolSize" -> "2"
 102 = {HashMap$Node@8869} "hive.spark.client.rpc.max.size" -> "52428800"
 103 = {HashMap$Node@8870} "spark.hadoop.hbase.thrift.maxQueuedRequests" -> "1000"
 104 = {HashMap$Node@8871} "spark.hadoop.hbase.rpc.timeout" -> "60000"
 105 = {HashMap$Node@8872} "spark.hadoop.hbase.hregion.preclose.flush.size" -> "5242880"
 106 = {HashMap$Node@8873} "spark.hadoop.hbase.regionserver.hlog.reader.impl" -> "org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader"
 107 = {HashMap$Node@8874} "spark.hadoop.hbase.table.lock.enable" -> "true"
 108 = {HashMap$Node@8875} "spark.hadoop.hbase.zookeeper.dns.interface" -> "default"
 109 = {HashMap$Node@8876} "spark.master" -> "local"
 110 = {HashMap$Node@8877} "spark.hadoop.hbase.thrift.minWorkerThreads" -> "16"
 111 = {HashMap$Node@8878} "spark.hadoop.hbase.ipc.server.callqueue.scan.ratio" -> "0"
 112 = {HashMap$Node@8879} "spark.hadoop.hbase.hstore.checksum.algorithm" -> "CRC32"
 113 = {HashMap$Node@8880} "spark.hadoop.hbase.master.loadbalancer.class" -> "org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer"
 114 = {HashMap$Node@8881} "spark.hadoop.zookeeper.znode.parent" -> "/hbase"
 115 = {HashMap$Node@8882} "spark.hadoop.hbase.coprocessor.abortonerror" -> "true"
 116 = {HashMap$Node@8883} "spark.hadoop.hbase.coordinated.state.manager.class" -> "org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager"
 117 = {HashMap$Node@8884} "spark.hadoop.hbase.hregion.majorcompaction.jitter" -> "0.50"
 118 = {HashMap$Node@8885} "spark.hadoop.hbase.hregion.memstore.mslab.enabled" -> "true"
 119 = {HashMap$Node@8886} "spark.hadoop.hbase.zookeeper.quorum" -> "localhost"
 120 = {HashMap$Node@8887} "spark.hadoop.hbase.storescanner.parallel.seek.enable" -> "false"
 121 = {HashMap$Node@8888} "spark.hadoop.hbase.regionserver.logroll.period" -> "3600000"
 122 = {HashMap$Node@8889} "spark.hadoop.hbase.ipc.server.callqueue.read.ratio" -> "0"
 123 = {HashMap$Node@8890} "spark.hadoop.hbase.column.max.version" -> "1"
 124 = {HashMap$Node@8891} "spark.hadoop.hbase.regionserver.dns.interface" -> "default"
 125 = {HashMap$Node@8892} "spark.hadoop.hbase.hstore.time.to.purge.deletes" -> "0"
 126 = {HashMap$Node@8893} "spark.hadoop.hbase.client.scanner.max.result.size" -> "2097152"
 127 = {HashMap$Node@8894} "spark.hadoop.hbase.master.catalog.timeout" -> "600000"
 128 = {HashMap$Node@8895} "spark.kryo.classesToRegister" -> "org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,org.apache.hadoop.io.Writable,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.io.HiveKey"
 129 = {HashMap$Node@8896} "hive.spark.client.rpc.threads" -> "8"
 130 = {HashMap$Node@8897} "spark.hadoop.hbase.hstore.compaction.max" -> "10"
 131 = {HashMap$Node@8898} "spark.hadoop.hbase.rs.cacheblocksonwrite" -> "false"
 132 = {HashMap$Node@8899} "spark.hadoop.hbase.regionserver.thrift.framed" -> "false"
 133 = {HashMap$Node@8900} "spark.hadoop.hbase.regions.slop" -> "0.2"
 134 = {HashMap$Node@8901} "spark.hadoop.hbase.cells.scanned.per.heartbeat.check" -> "10000"
 135 = {HashMap$Node@8902} "spark.serializer" -> "org.apache.spark.serializer.KryoSerializer"
 136 = {HashMap$Node@8903} "spark.hadoop.hbase.ipc.client.tcpnodelay" -> "true"
 137 = {HashMap$Node@8904} "spark.hadoop.hbase.hregion.memstore.flush.size" -> "134217728"
 138 = {HashMap$Node@8905} "spark.hadoop.hbase.zookeeper.property.maxClientCnxns" -> "300"
 139 = {HashMap$Node@8906} "spark.hadoop.hbase.regionserver.info.bindAddress" -> "0.0.0.0"
 140 = {HashMap$Node@8907} "spark.hadoop.hbase.client.max.perserver.tasks" -> "5"
 141 = {HashMap$Node@8908} "spark.hadoop.hbase.master.logcleaner.ttl" -> "600000"
 142 = {HashMap$Node@8909} "spark.hadoop.hbase.status.published" -> "false"
 143 = {HashMap$Node@8910} "spark.hadoop.hbase.rest.support.proxyuser" -> "false"
 144 = {HashMap$Node@8911} "spark.hadoop.hbase.regionserver.info.port.auto" -> "false"
 145 = {HashMap$Node@8912} "spark.hadoop.hbase.metrics.exposeOperationTimes" -> "true"
 146 = {HashMap$Node@8913} "spark.hadoop.hbase.snapshot.restore.failsafe.name" -> "hbase-failsafe-{snapshot.name}-{restore.timestamp}"
 147 = {HashMap$Node@8914} "spark.hadoop.hbase.zookeeper.property.dataDir" -> "/tmp/hbase-shg4873/zookeeper"
 148 = {HashMap$Node@8915} "spark.hadoop.hbase.bulkload.retries.number" -> "10"
 149 = {HashMap$Node@8916} "spark.hadoop.hbase.regionserver.handler.abort.on.error.percent" -> "0.5"
 150 = {HashMap$Node@8917} "spark.hadoop.hbase.auth.key.update.interval" -> "86400000"
 151 = {HashMap$Node@8918} "spark.hadoop.hbase.security.visibility.mutations.checkauths" -> "false"
 152 = {HashMap$Node@8919} "spark.hadoop.hbase.regionserver.thrift.compact" -> "false"
 153 = {HashMap$Node@8920} "spark.hadoop.hbase.client.keyvalue.maxsize" -> "10485760"
 154 = {HashMap$Node@8921} "spark.hadoop.hbase.status.publisher.class" -> "org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher"
 155 = {HashMap$Node@8922} "spark.hadoop.hbase.regionserver.msginterval" -> "3000"
 156 = {HashMap$Node@8923} "spark.hadoop.hbase.regionserver.checksum.verify" -> "true"