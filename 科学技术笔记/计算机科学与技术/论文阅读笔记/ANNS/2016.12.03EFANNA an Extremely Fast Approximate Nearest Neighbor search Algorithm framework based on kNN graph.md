# EFANNA: an Extremely Fast Approximate Nearest Neighbor search Algorithm framework based on kNN graph

# 1 导论

基于图的方法最本质的思想是：一个邻居的邻居也可能是邻居，也称之为邻居扩展NN-expansion。

离线构建一个k邻近图（k-nearest neighbor，kNN）。

首先以一种方式（如随机选择）找一批差查询点的邻居（最初是个假邻居），然后依次检查这些邻居的邻居，保留与差选点更近的点。

有什么问题？

导致局部最优，低召回率。针对这个问题，如下论文改进了初始邻居的选择方法，使用hash方法选择，相比随机选择有大幅改善。

[22] Z. Jin, D. Zhang, Y. Hu, S. Lin, D. Cai, and X. He. Fast and accurate hashing via iterative nearest neighbors expansion. IEEE transactions on cybernetics, 44(11):2167–2177, 2014.

另外一挑战是什么？

构建一个kNN图需要很高的计算代价，尤其数据量大的情况。如下论文改进了kNN图的构建性能。

[4] J. L. Bentley. Multidimensional divide-and-conquer. Communications of the Acm, 23(4):214–229, 1980.

[9] K. L. Clarkson. Fast algorithms for the all nearest neighbors problem. Foundations of Computer Science Annual Symposium on, pages 226–232, 1983.

[32] P. M. Vaidya. An o(nlogn) algorithm for the all-nearest-neighbors problem. Discrete & Computational Geometry, 4(2):101–115, 1989.

[28] P. C. of k-Nearest Neighbor Graphs in Metric Spaces. Paredes, rodrigo and chavez, edgar and figueroa, karina and navarro, gonzalo. In International Workshop on Experimental and Efficient Algorithms, pages 85–97, 2006.

然而，这些方法在大数据场景仍然不够高效。这些方法都是构建精准kNN图。如下文章尝试通过构建近似kNN改善性能。

NN-expansion

NN-descent

[12] W. Dong, C. Moses, and K. Li. Efficient k-nearest neighbor graph construction for generic similarity measures. In Proceedings of the 20th international Conference on World Wide Web, pages 577–586, 2011.

divide-and-conquer

[8] J. Chen, H. R. Fang, and Y. Saad. Fast approximate knn graph construction for high dimensional data via recursive lanczos bisection. Journal of Machine Learning Research, 10(Sep):1989–2012, 2009.

[16] R. Gan, J. Wang, J. Wang, G. Zeng, Z. Tu, and S. Li. Scalable knn graph construction for visual descriptors. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 1106–1113, 2012.

[37] Y.-m. Zhang, K. Huang, G. Geng, and C.-l. Liu. Fast knn graph construction with locality sensitive hashing. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 660–674. Springer, 2013.

[31] J. Tang, J. Liu, M. Zhang, and Q. Mei. Visualizing large-scale and high-dimensional data. In Proceedings of the 25th International Conference on World Wide Web, pages 287–297, 2016.

这些方法的原理：

第一步：将整个数据集分解成很多小的数据集。

第二步：通过蛮力搜索小的数据集构建很多互相之间有重叠的子kNN图。

第三步：合并所有的子kNN图，再用类似于NN-expansion的方法修正。

尽管近似kNN图可以高效地构建了，没有正式的研究对比在近似kNN图和精准kNN图上进行ANN搜索的性能。本文对此进行了研究。

我们的算法基于一个简单的观察：NN-expansion和NN-descent方法的搜索性能对初始化非常敏感，好的初始化导致高性能，差的初始化导致低性能。

我们的索引包含两部分：多层随机层次化结构（如随机剪枝KD-树）。近似kNN图。

在离线阶段

首先，多次分割数据集成大量子集，产生多层随机的层次化结构；然后，沿着层次结构，从底向上处理。在处理阶段，利用数据结构定位可能的最邻近邻居，用这些候选邻居更新图，相比使用子树中的所有节点开销大幅减少。最后，使用类似NN-descent方法修正图，基于NN-expansion思想，引入local join、sampling、early termination优化技术[12]。

在在线搜索阶段

首先在层次结构中获取一个查询点的候选邻居，然后在kNN图中使用NN-expansion修正结果。

本文的贡献

EFANNA在索引大小、搜索速度、搜索精度上优胜于Flann方法，这是一个很流行的方法，该方法的论文如下：

[26] M. Muja and D. G. Lowe. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(11):2227–2240, 2014.

在百万级的数据上，EFANNA构建近似kNN图的速度是蛮力法的几百倍。如下论文研究了基于kNN图的无监督和半监督学习，我们的方法为其提供了更大规模数据集上的可能。

[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research, 7(Nov):2399–2434, 2006.

[29] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888–905, 2000.

发现在精度低的近似kNN图上进行ANN搜索仍然表现非常好，这是因为错的邻居就是稍微远一点点的邻居。

# 2 相关工作

# 3 用于ANN搜索的EFANNA算法

## 3.1 用EFANNA索引进行ANN搜索

主要思想上是获取一个更好的初始化改善NN-expansion的性能。

很多可能的层次结构，如下

层次聚类

[26] M. Muja and D. G. Lowe. Scalable nearest neighbor algorithms for high dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(11):2227–2240, 2014.

随机划分树

[10] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 537–546. ACM, 2008.

本文中使用randomized truncated KD-tree，3.2节会讨论它与traditional randomized KD-tree的不同。如下论文研究了traditional randomized KD-tree。

[30] C. Silpa-Anan and R. Hartley. Optimised kd-trees for fast image descriptor matching. In Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8, 2008.

三个重要的参数：E，扩展因子；P，候选池子大小；I，迭代次数。

研究发现I=4足够了。更大的E和更大的P牺牲了搜索速度，获得了搜索精度。

## 3.2 EFANNA索引构建算法1：树构建

```
void DFSbuild(Node* node, std::mt19937& rng, unsigned* indices, unsigned count, unsigned offset){
	//omp_set_lock(&rootlock);
	//std::cout<<node->treeid<<":"<<offset<<":"<<count<<std::endl;
	//omp_unset_lock(&rootlock);
	if(count <= params_.TNS){
		node->DivDim = -1;
		node->Lchild = NULL;
		node->Rchild = NULL;
		node->StartIdx = offset;
		node->EndIdx = offset + count;
		//add points

	}else{
		unsigned idx;
		unsigned cutdim;
		DataType cutval;
		meanSplit(rng, indices, count, idx, cutdim, cutval);
		node->DivDim = cutdim;
		node->DivVal = cutval;
		node->StartIdx = offset;
		node->EndIdx = offset + count;
		Node* nodeL = new Node(); Node* nodeR = new Node();
		node->Lchild = nodeL;
		nodeL->treeid = node->treeid;
		DFSbuild(nodeL, rng, indices, idx, offset);
		node->Rchild = nodeR;
		nodeR->treeid = node->treeid;
		DFSbuild(nodeR, rng, indices+idx, count-idx, offset+idx);
	}
}
```

```
void buildTrees(){
	unsigned N = features_.get_rows();
	unsigned seed = 1998;
	std::mt19937 rng(seed);
	nhoods.resize(N);
	g.resize(N);
	boost::dynamic_bitset<> visited(N, false);
	knn_graph.resize(N);
	for (auto &nhood: nhoods) {
		//nhood.nn_new.resize(params_.S * 2);
		nhood.pool.resize(params_.L+1);
		nhood.radius = std::numeric_limits<float>::max();
	}


	//build tree
	std::vector<int> indices(N);
	LeafLists.resize(TreeNum);
	std::vector<Node*> ActiveSet;
	std::vector<Node*> NewSet;
	for(unsigned i = 0; i < (unsigned)TreeNum; i++){
		Node* node = new Node;
		node->DivDim = -1;
		node->Lchild = NULL;
		node->Rchild = NULL;
		node->StartIdx = 0;
		node->EndIdx = N;
		node->treeid = i;
		tree_roots_.push_back(node);
		ActiveSet.push_back(node);
	}
#pragma omp parallel for
	for(unsigned i = 0; i < N; i++)indices[i] = i;
#pragma omp parallel for
	for(unsigned i = 0; i < (unsigned)TreeNum; i++){
		std::vector<unsigned>& myids = LeafLists[i];
		myids.resize(N);
		std::copy(indices.begin(), indices.end(),myids.begin());
		std::random_shuffle(myids.begin(), myids.end());
	}
	omp_init_lock(&rootlock);
	while(!ActiveSet.empty() && ActiveSet.size() < 1100){
#pragma omp parallel for
		for(unsigned i = 0; i < ActiveSet.size(); i++){
			Node* node = ActiveSet[i];
			unsigned mid;
			unsigned cutdim;
			DataType cutval;
			std::mt19937 rng(seed ^ omp_get_thread_num());
			std::vector<unsigned>& myids = LeafLists[node->treeid];

			meanSplit(rng, &myids[0]+node->StartIdx, node->EndIdx - node->StartIdx, mid, cutdim, cutval);

			node->DivDim = cutdim;
			node->DivVal = cutval;
			//node->StartIdx = offset;
			//node->EndIdx = offset + count;
			Node* nodeL = new Node(); Node* nodeR = new Node();
			nodeR->treeid = nodeL->treeid = node->treeid;
			nodeL->StartIdx = node->StartIdx;
			nodeL->EndIdx = node->StartIdx+mid;
			nodeR->StartIdx = nodeL->EndIdx;
			nodeR->EndIdx = node->EndIdx;
			node->Lchild = nodeL;
			node->Rchild = nodeR;
			omp_set_lock(&rootlock);
			if(mid>params_.S)NewSet.push_back(nodeL);
			if(nodeR->EndIdx - nodeR->StartIdx > params_.S)NewSet.push_back(nodeR);
			omp_unset_lock(&rootlock);
		}
		ActiveSet.resize(NewSet.size());
		std::copy(NewSet.begin(), NewSet.end(),ActiveSet.begin());
		NewSet.clear();
	}
#pragma omp parallel for
	for(unsigned i = 0; i < ActiveSet.size(); i++){
		Node* node = ActiveSet[i];
		//omp_set_lock(&rootlock);
		//std::cout<<i<<":"<<node->EndIdx-node->StartIdx<<std::endl;
		//omp_unset_lock(&rootlock);
		std::mt19937 rng(seed ^ omp_get_thread_num());
		std::vector<unsigned>& myids = LeafLists[node->treeid];
		DFSbuild(node, rng, &myids[0]+node->StartIdx, node->EndIdx-node->StartIdx, node->StartIdx);
	}
}
```

## 3.3 EFANNA索引构建算法1：近似kNN图构建

### 3.3.1 层次化随机分而治之

```
void initGraph(){
	//initial
	unsigned N = features_.get_rows();
	unsigned seed = 1998;
	std::mt19937 rng(seed);
	nhoods.resize(N);
	g.resize(N);
	boost::dynamic_bitset<> visited(N, false);
	knn_graph.resize(N);
	for (auto &nhood: nhoods) {
		//nhood.nn_new.resize(params_.S * 2);
		nhood.pool.resize(params_.L+1);
		nhood.radius = std::numeric_limits<float>::max();
	}


	//build tree
	std::vector<int> indices(N);
	LeafLists.resize(TreeNum);
	std::vector<Node*> ActiveSet;
	std::vector<Node*> NewSet;
	for(unsigned i = 0; i < (unsigned)TreeNum; i++){
		Node* node = new Node;
		node->DivDim = -1;
		node->Lchild = NULL;
		node->Rchild = NULL;
		node->StartIdx = 0;
		node->EndIdx = N;
		node->treeid = i;
		tree_roots_.push_back(node);
		ActiveSet.push_back(node);
	}
#pragma omp parallel for
	for(unsigned i = 0; i < N; i++)indices[i] = i;
#pragma omp parallel for
	for(unsigned i = 0; i < (unsigned)TreeNum; i++){
		std::vector<unsigned>& myids = LeafLists[i];
		myids.resize(N);
		std::copy(indices.begin(), indices.end(),myids.begin());
		std::random_shuffle(myids.begin(), myids.end());
	}
	omp_init_lock(&rootlock);
	while(!ActiveSet.empty() && ActiveSet.size() < 1100){
#pragma omp parallel for
		for(unsigned i = 0; i < ActiveSet.size(); i++){
			Node* node = ActiveSet[i];
			unsigned mid;
			unsigned cutdim;
			DataType cutval;
			std::mt19937 rng(seed ^ omp_get_thread_num());
			std::vector<unsigned>& myids = LeafLists[node->treeid];

			meanSplit(rng, &myids[0]+node->StartIdx, node->EndIdx - node->StartIdx, mid, cutdim, cutval);

			node->DivDim = cutdim;
			node->DivVal = cutval;
			//node->StartIdx = offset;
			//node->EndIdx = offset + count;
			Node* nodeL = new Node(); Node* nodeR = new Node();
			nodeR->treeid = nodeL->treeid = node->treeid;
			nodeL->StartIdx = node->StartIdx;
			nodeL->EndIdx = node->StartIdx+mid;
			nodeR->StartIdx = nodeL->EndIdx;
			nodeR->EndIdx = node->EndIdx;
			node->Lchild = nodeL;
			node->Rchild = nodeR;
			omp_set_lock(&rootlock);
			if(mid>params_.S)NewSet.push_back(nodeL);
			if(nodeR->EndIdx - nodeR->StartIdx > params_.S)NewSet.push_back(nodeR);
			omp_unset_lock(&rootlock);
		}
		ActiveSet.resize(NewSet.size());
		std::copy(NewSet.begin(), NewSet.end(),ActiveSet.begin());
		NewSet.clear();
	}
#pragma omp parallel for
	for(unsigned i = 0; i < ActiveSet.size(); i++){
		Node* node = ActiveSet[i];
		//omp_set_lock(&rootlock);
		//std::cout<<i<<":"<<node->EndIdx-node->StartIdx<<std::endl;
		//omp_unset_lock(&rootlock);
		std::mt19937 rng(seed ^ omp_get_thread_num());
		std::vector<unsigned>& myids = LeafLists[node->treeid];
		DFSbuild(node, rng, &myids[0]+node->StartIdx, node->EndIdx-node->StartIdx, node->StartIdx);
	}
	//DFStest(0,0,tree_roots_[0]);
	//build tree completed

	for(size_t i = 0; i < (unsigned)TreeNumBuild; i++){
		getMergeLevelNodeList(tree_roots_[i], i ,0);
	}

#pragma omp parallel for	
	for(size_t i = 0; i < mlNodeList.size(); i++){
		mergeSubGraphs(mlNodeList[i].second, mlNodeList[i].first);
	}


#pragma omp parallel
	{
#ifdef _OPENMP
		std::mt19937 rng(seed ^ omp_get_thread_num());
#else
		std::mt19937 rng(seed);
#endif
		std::vector<unsigned> random(params_.S + 1);

#pragma omp for
		for (unsigned n = 0; n < N; ++n) {
			auto &nhood = nhoods[n];
			Points &pool = nhood.pool;
			if(nhood.nn_new.size()<params_.S*2){
				nhood.nn_new.resize(params_.S*2);
				GenRandom(rng, &nhood.nn_new[0], nhood.nn_new.size(), N);
			}


			GenRandom(rng, &random[0], random.size(), N);
			nhood.L = params_.S;
			nhood.Range = params_.S;
			while(knn_graph[n].size() < params_.S){
				unsigned rand_id = rng() % N;
				DataType dist = distance_->compare(
						features_.get_row(n), features_.get_row(rand_id), features_.get_cols());
				Candidate<DataType> c(rand_id,dist);
				knn_graph[n].insert(c);
			}

			//omp_set_lock(&rootlock);
			//if(knn_graph[n].size() < nhood.L)std::cout<<n<<":"<<knn_graph[n].size()<<std::endl;
			//omp_unset_lock(&rootlock);
			unsigned i = 0;
			typename CandidateHeap::reverse_iterator it = knn_graph[n].rbegin();
			for (unsigned l = 0; l < nhood.L; ++l) {
				if (random[i] == n) ++i;
				auto &nn = nhood.pool[l];
				nn.id = it->row_id;//random[i++];
				nhood.nn_new[l] = it->row_id;
				nn.dist = it->distance;//distance_->compare(features_.get_row(n), features_.get_row(nn.id), features_.get_cols());
				nn.flag = true;it++;
				//if(it == knn_graph[n].rend())break;
			}
			sort(pool.begin(), pool.begin() + nhood.L);
		}
	}
	knn_graph.clear();
#ifdef INFO
	std::cout<<"initial completed"<<std::endl;
#endif
}
```

## 3.3.2 图修正

```
void refineGraph(){
	std::cout << " refineGraph" << std::endl;
	int iter = 0;
	clock_t s,f;
	s = clock();unsigned int l=100;
	while(iter++ < params_.build_epoches){
		join();//std::cout<<"after join"<<std::endl;
		update(l);
		f = clock();
		std::cout << "iteration "<< iter << " time: "<< (f-s)*1.0/CLOCKS_PER_SEC<<" seconds"<< std::endl;
	}
	//calculate_norm();
	std::cout << "saving graph" << std::endl;
	/*    knn_graph.clear();

	  for(size_t i = 0; i < nhoods.size(); i++){
	    CandidateHeap can;
	    for(size_t j = 0; j < params_.K; j++){
	      Candidate<DataType> c(nhoods[i].pool[j].id,nhoods[i].pool[j].dist);
	      can.insert(c);
	  
	    }
	    while(can.size()<params_.K){
	      unsigned id = rand() % nhoods.size();
	      DataType dist = distance_->compare(features_.get_row(i), features_.get_row(id),features_.get_cols());
	      Candidate<DataType> c(id, dist);
	      can.insert(c);
	    }
	    knn_graph.push_back(can);
	  }
	*/
	g.resize(nhoods.size());
	M.resize(nhoods.size());
	gs.resize(nhoods.size());
	for(unsigned i = 0; i < nhoods.size();i++){
	    M[i] = nhoods[i].Range;
	    g[i].resize(nhoods[i].pool.size());
	    std::copy(nhoods[i].pool.begin(), nhoods[i].pool.end(), g[i].begin());
	    gs[i].resize(params_.K);
	    for(unsigned j = 0; j < params_.K;j++)
		gs[i][j] = g[i][j].id;
	}
}
```


代码解读

std::vector<Neighbor>  nhoods;

neighborhood



```
union ValueType{
    int int_val;
    float float_val;
    char* str_pt;
};

typedef std::map<std::string, ValueType> ExtraParamsMap;
struct IndexParams{
    init_algorithm init_index_type;
    size_t K;  //build knn table with nn = K
    size_t S;  //nn sets' max size
    size_t L =30;//rnn size
    size_t TNS = 10;//tree node size
    size_t Check_K = 40; // 修正图的参数
    int build_epoches; // 修正图的参数，最大迭代次数
    int extend_num; //number to extend each time
    size_t pool_size = 100; // 修正图的参数
    size_t init_num = 100;
    ExtraParamsMap extra_params;
    bool reverse_nn_used;
};

struct SearchParams{
    int search_init_num;
    int search_epoches;
    int search_method;
    unsigned extend_to;
    unsigned tree_num;
    unsigned search_depth;
};


template <typename DataType>
class InitIndex{
    const Matrix<DataType> features_;
    const Distance<DataType>* distance_;
    const IndexParams params_;
    std::vector<std::vector<int> > knn_table_gt;
    std::vector<std::vector<Point> > g;
    std::vector<std::vector<unsigned> > gs;
    std::vector<unsigned> M;
    std::vector<CandidateHeap> knn_graph;
    std::vector<DataType> norms;
    std::vector<std::vector<int> > nn_results;
    DataType* Radius;
};
typedef std::set<Candidate<DataType>, std::greater<Candidate<DataType>> > CandidateHeap;

template<typename T>
struct Candidate {
    size_t row_id;
    T distance;
};

struct Neighbor {
    std::shared_ptr<Lock> lock;
    float radius; // 最远的邻居的距离
    float radiusM;
    Points pool;
    unsigned L; // 当前的邻居数量
    unsigned Range;
    bool found;
    std::vector<unsigned> nn_old;
    std::vector<unsigned> nn_new;
    std::vector<unsigned> rnn_old;
    std::vector<unsigned> rnn_new;
};

typedef std::vector<Point>  Points;

struct Point {
    unsigned id; // 当前点的唯一id
    float dist; // 到另一个点的距离
    bool flag;
}

```
